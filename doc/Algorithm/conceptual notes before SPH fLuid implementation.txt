Overall SPH Fluid Simulation - Conceptual Notes
================================================

TODO research:
------------------
  - Z-Index formula (Goswami 2010 -> Mor66)
  
  - first hints for parallel radix sort parallel prefix sum + parallel reduction:
    (maybe not that complete, but the whole of all algos will be extractable from all my collected papers+
     printed thesises + gpu gems) : 
      - http://http.developer.nvidia.com/GPUGems3/gpugems3_ch32.html <-- basic paper for parallel scan with CUDA
      - http://http.developer.nvidia.com/GPUGems3/gpugems3_ch39.html <-- application for broad phase coll. detection 
      
  - refresh knowledge about efficient atomicMin/Max/Inc etc. calculation patterns:
    - http://supercomputingblog.com/cuda/cuda-tutorial-5-performance-of-atomics/
    
          
 - refresh work efficient parallel prefix sum, especially the avoidance of bank conflicts:
    ERROR in GPU gems paper:
       1. #define NUM_BANKS 16  
       2. #define LOG_NUM_BANKS 4  
       3. #define CONFLICT_FREE_OFFSET(n) \  
       4.     ((n) >> NUM_BANKS + (n) >> (2 * LOG_NUM_BANKS))  
       
      must be:
       1. #define NUM_BANKS 16  
       2. #define LOG_NUM_BANKS 4  
       3. #define CONFLICT_FREE_OFFSET(n) \  
       4.     ((n) >> NUM_BANKS + (n) >> (LOG_NUM_BANKS))  
       because padding must be every 16 elements for non-fermi GPUs
       
       The division by 2^16 is also bullshit as we never can have more than 1024 threads on non-fermi GPUs 
       (I dont' understand this strange offset anyway, even if n would be possible to be greater than 65k).
       Hence:
       1. #define NUM_BANKS 16  
       2. #define LOG_NUM_BANKS 4  
       3. #define CONFLICT_FREE_OFFSET(n) \  
       4.     ( (n) >> (LOG_NUM_BANKS) )
       
       For Fermi GPUs:
       1. #define NUM_BANKS 32
       2. #define LOG_NUM_BANKS 5
       3. #define CONFLICT_FREE_OFFSET(n) \  
       4.     ( (n) >> (LOG_NUM_BANKS) )        
         
  
  
References for Algorithms:
--------------------------
  - rough voxelization per depth peeling: http://http.developer.nvidia.com/GPUGems3/gpugems3_ch29.html
  
 
 
 
 
Parallel Prefix Sum concepts:
-----------------------------
  - Parallel Prefix Sum:
  ~~~~~~~~~~~~~~~~~~~~~~
    - scheme: 
      - Kernel1 :
          - parallel scan locally on work group scale, write sum to globalScanArray:
            -(TODO refine)
      - Kernel2: 
          - one single work group scans globalScanArray; (we are finished if only the total sum is of interest)
      - Kernel3: 
          - if total prefix sum is of interest: 
              - launch as many work groups as in kernel 1:
                - every work group grabs one item of globalScanArray and adds this value to every item 
                  in its range within the actual scanResultArray;
      
  - Parallel Reduction:
  ~~~~~~~~~~~~~~~~~~~~~
    - scheme:
      - do a parallel prefix sum (see above) in order to calculate the new array indices for each element in originalArray;
      - Kernel4: 
        - if(isValid(currentElement)):
           compactedElementArray[ scanResultArray[ currentElementIndex] ] = currentElement;
           
  - Parallel Radix Sort:
  ~~~~~~~~~~~~~~~~~~~~~~
   
    - TODO
  
  
Simulation Algorithm:
---------------------
  Phase I: Update Accleration Structure:
  --------------------------------------
  
    (( only initially, usually done in integration kernel: 
      - Kernel calculateZIndexKernel:
        - calculate z-Index for each particle, write to zIndexBuffer ))
    
    - Kernel parallelRadixSortKernel (actually three kernels):
      - sort particles according to z-Index via Parallel Radix Sort 
      
    - Kernel reorderAttributes
         - (included here: rigid body meta data calculation (backtracking of changing particle positions in reordered array) )
    
  
    - Kernel updateUniformGrid
      - for each grid cell (initialized to contain zero particles) in parallel:
        - if cell not empty: 
            - write out (particle start index) and (particle end index +1); reason: much more efficient on current hardware
              than atomic writes for "direct" particle count write-out; "fix" the side effects of this optimization in next kernel
     
     
    //split and compress unifiorm grid 
      - Kernel scan_localPar_globalSeq 
          - tabulate: 0 particles --> 0; 1..32 paricles -->1 etc.; 
            Do the "fix" of the optimization in the updateUniformGrid kernel within the tabulation function: 
            instead of reading numParticles directly, we have to compute it:
            numParticles = (partEndIndexPlus1>0) ? (partEndIndexPlus1-partStartIndex) : 0; 
            and write it back;
          - nearly-global scan on tabulated values; 
          
      - Kernel splitAndCompactUniformGrid:
          - finish global scan
          - compact:  0 particles --> don't write out; 
                     >0 particles --> write out at scan offset, numParticles/maxPartsPerCell elements,
                                      adjust particle count and particles start index
                             
    //actual SPH physics simulation:                                      
      - Kernel updateDensity;
     
      (( only initially: Kernel _initial_updateForce_integrate_calcZIndex;
         do reuse all code from updateForce_integrate_calcZIndex kernel but the integration code; we need a simplified first integraation step
         to initialize for the "dual valued" Velocity Verlet integration scheme))
      - Kernel updateForce_integrate_calcZIndex;
        do every particle-related SPH stuff but the density: force calculations, integration, z index calculation;
        
    //Rigid body extra treatment:
      - Kernel updateRigidBodies;
      
    
    
    
    

  -----------------------------------
    (todo copy from hand written notes;)
    (note: integration via Velocity Verlet method, as because we need explicit velocities, leap frog is not applicable,
    i.e. two evalutations for both position and velocitiy and force have to be done per simulation step; Regarding this fact,
    Velocity Verlet is slightly more accurate at the same computational cost; 
    [David H. Eberly: Game Physics; page 483 f. ยง9.8.5 equation 9.35])
  
